import os
import numpy
import numpy as np
import scipy
import unittest

import csep.core.poisson_evaluations as poisson
import csep.core.binomial_evaluations as binary
import csep.core.brier_evaluations as brier
def get_datadir():
    root_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(root_dir, 'artifacts', 'Comcat')
    return data_dir


class TestPoissonLikelihood(unittest.TestCase):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.seed = 0
        numpy.random.seed(self.seed)
        # used for posterity
        self.random_matrix = numpy.random.rand(1, 4)
        self.forecast_data = numpy.array([[1, 1], [1, 1]])
        self.observed_data = numpy.array([[1, 1], [1, 1]])

    def test_simulate_catalog(self):
        # expecting the sampling weights to be [0.25, 0.5, 0.75, 1.0]
        # assuming the random numbers are equal to the following:
        random_numbers = numpy.array([[0.5488135, 0.71518937, 0.60276338, 0.54488318]])

        num_events = 4

        # ensures that our random numbers used to manually create expected observed_catalog are consistent
        numpy.testing.assert_allclose(random_numbers, self.random_matrix)

        # assuming that our forecast_data are uniform as defined above
        # bin[0] = [0, 0.25)
        # bin[1] = [0.25, 0.5)
        # bin[2] = [0.5, 0.75)
        # bin[3] = [0.75, 1.0)
        expected_catalog = [0, 0, 4, 0]

        # forecast_data should be event counts
        expected_forecast_count = numpy.sum(self.forecast_data)

        # used to determine where simulated earthquake shoudl be placed
        sampling_weights = numpy.cumsum(self.forecast_data.ravel()) / expected_forecast_count

        # this is taken from the test likelihood function
        sim_fore = numpy.empty(sampling_weights.shape)
        sim_fore = poisson._simulate_catalog(num_events, sampling_weights, sim_fore,
                                     random_numbers=self.random_matrix)

        # final statement
        numpy.testing.assert_allclose(expected_catalog, sim_fore)

        # test again to ensure that fill works properply
        sim_fore = poisson._simulate_catalog(num_events, sampling_weights, sim_fore,
                                     random_numbers=self.random_matrix)

        # final statement
        numpy.testing.assert_allclose(expected_catalog, sim_fore)

    def test_likelihood(self):
        qs, obs_ll, simulated_ll = poisson._poisson_likelihood_test(self.forecast_data, self.observed_data, num_simulations=1,
                                                            random_numbers=self.random_matrix, use_observed_counts=True)

        # very basic result to pass "laugh" test
        numpy.testing.assert_allclose(qs, 1)

        # forecast and observation are the same, sum(np.log(poisson(1, 1))) = -4
        numpy.testing.assert_allclose(obs_ll, -4)

        # calculated by hand given the expected data, see explanation in zechar et al., 2010.
        numpy.testing.assert_allclose(simulated_ll[0], -7.178053830347945)


class TestBinomialLikelihood(unittest.TestCase):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.seed = 0
        numpy.random.seed(self.seed)
        self.forecast_data = numpy.array([[0.1, 0.3, 0.4], [0.2, 0.1, 0.1]])
        self.observed_data = numpy.array([[0, 1, 2], [1, 1, 0]])
        self.random_matrix = numpy.random.rand(1, 9)

    def test_joint_likelihood_calculation(self):
        bill = binary.binary_joint_log_likelihood_ndarray(self.forecast_data, self.observed_data)
        numpy.testing.assert_allclose(bill, -6.7197988064)

    def test_simulate_active_cells(self):
        #With fixed seed we get the same random numbers if we get all the number at once or one by one.
        #Making sure random number generated by seed 0 match.
        expected_random_numbers = numpy.array([[0.5488135, 0.71518937, 0.60276338, 0.54488318, 0.4236548, 0.64589411,
                             0.4375872112626925, 0.8917730007820798, 0.9636627605010293]])

        numpy.testing.assert_allclose(expected_random_numbers, self.random_matrix)

        #We can expect the following catalog, if we get the above random numbers.
        #We get 4 active cells after 9th random sample.
        expected_catalog = [0, 0, 1, 1, 1, 1]

        sampling_weights = numpy.cumsum(self.forecast_data.ravel()) / numpy.sum(self.forecast_data)
        sim_fore = numpy.zeros(sampling_weights.shape)
        obs_active_cells =  len(numpy.unique(numpy.nonzero(self.observed_data.ravel())))
        #resetting seed again to 0, to make sure _simulate_catalog uses this.
        seed = 0
        numpy.random.seed(seed)
        sim_fore = binary._simulate_catalog(obs_active_cells, sampling_weights, sim_fore)
        numpy.testing.assert_allclose(expected_catalog, sim_fore)

    def test_binomial_likelihood(self):
        qs, bill, simulated_ll = binary._binary_likelihood_test(self.forecast_data,self.observed_data, num_simulations=1,seed=0, verbose=True)
        numpy.testing.assert_allclose(bill, -6.7197988064)
        numpy.testing.assert_allclose(qs, 1)
        numpy.testing.assert_allclose(simulated_ll[0], -7.921741654647629)


class TestPoissonBrier(unittest.TestCase):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.seed = 1
        self.forecast_data = numpy.array([[0.3, 0.2, 0.1], [0.2, 0.1, 0.1]])
        self.observed_data = numpy.array([[0, 1, 2], [1, 1, 0]])

    def test_brier_score_calculation(self):

        # 1 bin
        rate = 1
        prob = 1 - scipy.stats.poisson.pmf(0, rate)
        brier_score_hit = -2 * (prob - 1)**2
        brier_score = brier._brier_score_ndarray(numpy.array([[rate]]),
                                                 numpy.array([[1]]))
        numpy.testing.assert_allclose(brier_score_hit, brier_score)

        brier_score_nohit = -2 * prob**2
        brier_score = brier._brier_score_ndarray(numpy.array([[rate]]),
                                                 numpy.array([[0]]))
        numpy.testing.assert_allclose(brier_score_nohit, brier_score)
        # 2 bins
        rate = np.array([1, 0.5])
        hits = np.array([0, 1])
        prob = 1 - scipy.stats.poisson.pmf(0, rate)
        brier_score_2bins = (-2 * (prob - hits)**2).sum() / len(rate)
        brier_score = brier._brier_score_ndarray(np.array(rate),
                                                 np.array([hits]))
        numpy.testing.assert_allclose(brier_score_2bins, brier_score)

        hits = np.array([0, 0])
        brier_score_2bins = (-2 * (prob - hits)**2).sum() / len(rate)
        brier_score = brier._brier_score_ndarray(np.array(rate),
                                                 np.array([hits]))
        numpy.testing.assert_allclose(brier_score_2bins, brier_score)

    def test_brier_test(self):

        expected_sim = numpy.array([1, 1, 1, 1, 0, 0])
        qs, obs_brier, simulated_brier = brier._brier_score_test(
            self.forecast_data,
            self.observed_data,
            num_simulations=1,
            seed=self.seed,
            verbose=True)

        probs = 1 - scipy.stats.poisson.pmf(0, self.forecast_data.ravel())
        sim_brier_analytic = ((-2 * (probs - expected_sim)**2).sum() /
                              len(probs))
        numpy.testing.assert_allclose(simulated_brier[0], sim_brier_analytic)
        obs_brier_analytic = (
            (-2 * (probs - self.observed_data.ravel().astype(bool))**2).sum() /
            len(probs))
        numpy.testing.assert_allclose(obs_brier, obs_brier_analytic)


if __name__ == '__main__':
    unittest.main()
